# Introduction
This project focuses on implementing linear regression and binary logistic regression models, trained on two datasets: the Infrared Thermography Temperature dataset (Dataset 1, for linear regression) and the CDC Diabetes Health Indicators dataset (Dataset 2, for logistic regression). For Dataset 1, a temperature interval with more data has a larger impact on the fitting equation. In Dataset 2, the target variable is binary, where 0 represents "no diabetes" and 1 represents "prediabetes or diabetes." Categorical features were handled, missing values were imputed, and feature scaling was applied.

Both models were trained on 80% of their respective datasets, with the remaining 20% used for testing. Key hyperparameters such as learning rate, momentum, and batch size were carefully tuned to optimize performance. The linear regression models were evaluated based on Mean Absolute Error (MAE), Mean Squared Error (MSE), R² score, and convergence speed. The logistic regression models were assessed using metrics including loss, accuracy, precision, recall, F1 score, and convergence speed. The linear regression model’s performance substantially declined with a high learning rate. For logistic regression, a comparison between models trained on raw and preprocessed data indicated the negative impact of class imbalance. The model may tend to classify most instances as the majority class, which increases accuracy but harms other metrics (e.g. recall, F1 score, etc.) for the minority class.

# Dataset
The Infrared Thermography dataset (1042 samples, 33 features) and the CDC Diabetes dataset (253,680 samples, 21 features) both have features with small means and standard deviations, except for "Humidity," which has a higher standard deviation. This suggests that most features have low variability, with values clustered around the mean. In dataset 2, the imbalanced class distribution can lead to increased prediction errors for the minority class, as the model tends to favor the majority class. This results in reduced performance on key metrics, especially for the minority class. The decision boundaries may be biased towards the majority class, resulting in less accurate classification for the minority class.

To improve model performance, for both datasets, categorical features were one-hot encoded, missing values imputed with the mean for numerical features, and features standardized using StandardScaler. For dataset 2, class imbalance was addressed with feature transformation and synthetic minority oversampling.

# Discussion and Conclusion

This project explored the performance of linear and logistic regression models using a variety of tests, including different training set sizes, batch sizes, and learning rates. For linear regression, increasing the training set size led to improved model performance, with reduced returns beyond a 60% split. Logistic regression gave stable performance across various batch sizes, though larger batch sizes reduced training time while slightly degrading model accuracy. The mini-batch SGD approach was found to generalize better to unseen data compared to the analytical solution, and selecting the right learning rate was key to balancing model convergence and stability. In logistic regression on imbalanced data, the decision boundary tends to be biased toward the majority class, making it crucial to preprocess the dataset to handle imbalance effectively.
In future work, regularization methods such as Ridge or Lasso regression could be applied to balance the influence of the features and improve generalization.
